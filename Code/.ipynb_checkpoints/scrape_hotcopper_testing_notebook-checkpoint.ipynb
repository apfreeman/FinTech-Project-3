{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "910215a4-1030-4c8f-bb6e-5e6420f19f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "import selenium\n",
    "import bs4 as bs\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import re\n",
    "import datetime as dt\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from urllib.request import Request, urlopen\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import Process\n",
    "from multiprocessing import Queue\n",
    "import concurrent.futures\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f45a9947-e137-4602-bd6d-5d693fe866ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Creds for PostgreSQL connection\n",
    "sql_username=os.getenv(\"sql_username\")\n",
    "sql_pwd=os.getenv(\"sql_pwd\")\n",
    "\n",
    "# Create a connection to the database\n",
    "engine = create_engine(f\"postgresql://{sql_username}:{sql_pwd}@localhost:5432/hot_copper_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f321dc-f6fb-46a6-91b2-c54a23312c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = ['QIP','RDG','FLC','CRR','HZR','POD','IPG']\n",
    "#ticker_list = ['QIP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d075ffff-cce3-4b26-9039-fbfc579717a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e52844-213c-48f0-b871-aa2e5e159a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for QIP.\n",
      "Getting data for RDG.\n",
      "Getting data for FLC.\n",
      "Getting data for CRR.\n",
      "Getting data for HZR.\n",
      "Getting data for POD.\n",
      "Getting data for IPG.\n"
     ]
    }
   ],
   "source": [
    "# Initiate for loop for all tickers\n",
    "for ticker in ticker_list:\n",
    "\n",
    "    # F string printed for each ticker\n",
    "    print(f\"Getting data for {ticker}.\")\n",
    "\n",
    "    # For loop for range, indicating number of pages to loop through within the stock forum\n",
    "    for count in range(1, 6):\n",
    "\n",
    "        # Create dynamic web page url\n",
    "        discussion_string_page = (\n",
    "            f\"https://hotcopper.com.au/asx/{ticker}/discussion/page-{count}\"\n",
    "        )\n",
    "\n",
    "        # Create an empty list for HREF links\n",
    "        links = []\n",
    "\n",
    "        # Open driver\n",
    "        driver = webdriver.Chrome()\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Open new page\n",
    "            driver.get(discussion_string_page)\n",
    "\n",
    "            # Wait for website to be properly opened\n",
    "            driver.implicitly_wait(5)\n",
    "\n",
    "            # Use beautiful soup to obtain all page source on curent page\n",
    "            soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "            # Find all tables on curent page\n",
    "            hc_tables = soup.find_all(\"table\")\n",
    "\n",
    "            # Find all href links in the website\n",
    "            for link in soup.findAll(\"a\"):\n",
    "                links.append(link.get(\"href\"))\n",
    "\n",
    "            # Filter href links for individual posts using list comprehension\n",
    "            filtered_links = [x for x in links if x != None and \"post_id=\" in x]\n",
    "\n",
    "            # Create a dataframe to join onto the primary dataframe\n",
    "            list_df = pd.DataFrame({\"HREF_Link\": filtered_links})\n",
    "            list_df = list_df.apply(lambda x: \"https://hotcopper.com.au\" + x)\n",
    "            # Read in data using pandas\n",
    "            hc_comp = pd.read_html(str(hc_tables))\n",
    "\n",
    "            # Obtain table data\n",
    "            hc_comp = hc_comp[0]\n",
    "\n",
    "            # Drop NA values\n",
    "            hc_comp = hc_comp.dropna()\n",
    "\n",
    "            # Remove unwanted columns\n",
    "            hc_comp = hc_comp.drop(\n",
    "                columns=[\n",
    "                    \"Forum\",\n",
    "                    \"View\",\n",
    "                    \"Comments Created with Sketch.\",\n",
    "                    \"Views Created with Sketch.\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # Rename columns\n",
    "            hc_comp.columns = [\"Ticker\", \"Subject\", \"Poster\", \"Likes\", \"Date\"]\n",
    "\n",
    "            # Replace date values with today's date if a time value is within date column\n",
    "            hc_comp.loc[\n",
    "                (hc_comp[\"Date\"].str.contains(\":\")), \"Date\"\n",
    "            ] = dt.datetime.now().strftime(\"%d/%m/%y\")\n",
    "\n",
    "            # Reset index\n",
    "            hc_comp = hc_comp.reset_index(drop=True)\n",
    "\n",
    "            # Join href links onto the dataframe\n",
    "            hc_comp = pd.concat([hc_comp, list_df], axis=1)\n",
    "\n",
    "            # Obtain number of counts in ticker\n",
    "            hc_comp[\"Ticker_Filter\"] = hc_comp[\"Ticker\"].apply(lambda x: len(x))\n",
    "\n",
    "            # Filter out for tickers not equal to count of 3\n",
    "            hc_comp = hc_comp[hc_comp[\"Ticker_Filter\"] == 3]\n",
    "\n",
    "            # Conditional for initial dataframe\n",
    "            if count == 1:\n",
    "\n",
    "                # Compile summary data\n",
    "                hc_stock_sum = hc_comp\n",
    "                \n",
    "                # Wait for website to be properly opened\n",
    "                driver.implicitly_wait(10)\n",
    "\n",
    "            # If not initial dataframe, concatenate data\n",
    "            else:\n",
    "\n",
    "                # Concatenate\n",
    "                hc_stock_sum = pd.concat([hc_stock_sum, hc_comp])\n",
    "                \n",
    "                # Wait for website to be properly opened\n",
    "                driver.implicitly_wait(10)\n",
    "                \n",
    "\n",
    "            # Append count total\n",
    "            count += 1\n",
    "        \n",
    "            # Add in sleep timer\n",
    "            time.sleep(5)\n",
    "\n",
    "        # Raise exception and flag error with stock\n",
    "        except Exception:\n",
    "\n",
    "            # Print error\n",
    "            print(f\"Error identified with {ticker}. Moving to next ticker.\")\n",
    "\n",
    "            # Continue through loop\n",
    "            continue\n",
    "    \n",
    "    # Write compiled summary data to DB\n",
    "    hc_stock_sum.to_sql('hc_stock_sum', con=engine, if_exists='append')\n",
    "    \n",
    "    try:\n",
    "        # Convert date to datetime\n",
    "        hc_stock_sum[\"Date\"] = pd.to_datetime(hc_stock_sum[\"Date\"], format=\"%d/%m/%y\")\n",
    "\n",
    "        # Capture daily comment and like count\n",
    "        primary_data[\"Daily_Comments\"] = primary_data.groupby([\"Date\", \"Ticker\"])[\n",
    "            \"Ticker\"\n",
    "        ].transform(\"count\")\n",
    "        primary_data[\"Daily_Likes\"] = primary_data.groupby([\"Date\", \"Ticker\"])[\n",
    "            \"Likes\"\n",
    "        ].transform(\"sum\")\n",
    "\n",
    "        # Create new column to determine if news source or individual poster\n",
    "        hc_stock_sum[\"Classifier\"] = hc_stock_sum[\"Poster\"].apply(\n",
    "            lambda x: \"ASX News\" if x == \"ASX News\" else \"Individual\"\n",
    "        )\n",
    "\n",
    "        # Conditions to append primary dataframe for all stocks\n",
    "        if stock_count == 0:\n",
    "\n",
    "            # Set initial datafram at first stock\n",
    "            primary_data = hc_stock_sum\n",
    "\n",
    "        # If not the first stock\n",
    "        elif stock_count > 0:\n",
    "\n",
    "            # Append the dataframe\n",
    "            primary_data = pd.concat([primary_data, hc_stock_sum])\n",
    "\n",
    "        # Add to count\n",
    "        stock_count += 1\n",
    "\n",
    "    except Exception:\n",
    "\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dea559f-6ea2-44d0-a3f8-2b154d7ae314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 360 entries, 0 to 71\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   Ticker         360 non-null    object        \n",
      " 1   Subject        360 non-null    object        \n",
      " 2   Poster         360 non-null    object        \n",
      " 3   Likes          360 non-null    float64       \n",
      " 4   Date           360 non-null    datetime64[ns]\n",
      " 5   HREF_Link      360 non-null    object        \n",
      " 6   Ticker_Filter  360 non-null    int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(4)\n",
      "memory usage: 22.5+ KB\n"
     ]
    }
   ],
   "source": [
    "hc_stock_sum.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ad45706-84ba-4e9a-93c7-16757c525402",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (scrape_asx_tickers.py, line 25)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\adam\\anaconda3\\envs\\project3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3444\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\adam\\AppData\\Local\\Temp\\2/ipykernel_5372/1378153317.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from scrape_asx_tickers import *\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\adam\\Workspace\\Projects\\FinTech-Project-3\\Code\\scrape_asx_tickers.py\"\u001b[1;36m, line \u001b[1;32m25\u001b[0m\n\u001b[1;33m    %reload_ext nb_black\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from scrape_asx_tickers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9c0217-e5e7-4feb-af1f-4bc8347f09a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
